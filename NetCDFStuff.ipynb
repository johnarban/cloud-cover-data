{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of netcdf files containing a Variable \"MOD08_D3_6_1_Cloud_Fraction_Day_Mean\"\n",
    "# want to perform some analysis on these files\n",
    "# and then output to a new netcdf file with the same dimensions and headers\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "# add type hints\n",
    "\n",
    "# define function to get the data from the netcdf file\n",
    "def get_data(filen: str):\n",
    "    \"\"\"\n",
    "    Get the data from the netcdf file\n",
    "    \"\"\"\n",
    "    # open the netcdf file\n",
    "    with nc.Dataset(filen) as data:\n",
    "    # get the data from the netcdf file\n",
    "        try:\n",
    "            var = data.variables['MOD08_D3_6_1_Cloud_Fraction_Day_Mean'][:]\n",
    "        except KeyError:\n",
    "            print(filen)\n",
    "            return None\n",
    "        # return the data\n",
    "        return var\n",
    "\n",
    "\n",
    "\n",
    "def analysis(data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Perform some analysis on the data\n",
    "    \"\"\"\n",
    "    # get the mean of the data\n",
    "    # mean = np.mean(data, axis=0)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    mean = np.ma.filled(mean, fill_value=-9999.0)\n",
    "    mean = np.ma.masked_equal(mean, -9999.0)\n",
    "    # return the mean\n",
    "    return mean\n",
    "\n",
    "def cloud_probability(data: np.ndarray, cloud_frac: float):\n",
    "    \"\"\"\n",
    "    Calculate the cloud probability\n",
    "    P(f > cloud_frac)\n",
    "    \"\"\"\n",
    "    # get the number of observations\n",
    "    n_obs = data.shape[0]\n",
    "    # get the number of observations where the cloud fraction is greater than the threshold\n",
    "    n_cloud = np.sum(data > cloud_frac, axis=0)\n",
    "    # calculate the cloud probability\n",
    "    cloud_prob = n_cloud / n_obs\n",
    "    # return the cloud probability\n",
    "    return cloud_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "# get the data from all the files\n",
    "filenames = glob.glob('data/scrub*.nc')\n",
    "for filen in filenames:\n",
    "    all_data.append(get_data(filen))\n",
    "    \n",
    "# convert the list to an array  \n",
    "all_data_m = np.ma.masked_array(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jal194/anaconda3/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1577: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    }
   ],
   "source": [
    "median = np.ma.median(all_data_m, axis=0)[0]\n",
    "mean = np.ma.mean(all_data_m, axis=0)[0]\n",
    "\n",
    "percentile = lambda x: np.ma.masked_invalid(np.nanpercentile(np.ma.filled(all_data_m, np.nan), x, axis=0))\n",
    "quantile_75 = percentile(75)\n",
    "quantile_25 = percentile(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to slice the data and create a new variable in the netcdf file\n",
    "def slice_data(dataset, var,latbounds, lonbounds, return_lat_lon=False):\n",
    "    \"\"\"\n",
    "    Slice the data and create a new variable in the netcdf file\n",
    "    \"\"\"\n",
    "    \n",
    "    lats = dataset.variables['lat'][:] \n",
    "    lons = dataset.variables['lon'][:]\n",
    "    \n",
    "    latbounds = list(sorted(latbounds))\n",
    "    lonbounds = list(sorted(lonbounds))\n",
    "\n",
    "    # latitude lower and upper index\n",
    "    latli = np.argmin( np.abs( lats - latbounds[0] ) )\n",
    "    latui = np.argmin( np.abs( lats - latbounds[1] ) ) \n",
    "\n",
    "    # longitude lower and upper index\n",
    "    lonli = np.argmin( np.abs( lons - lonbounds[0] ) )\n",
    "    lonui = np.argmin( np.abs( lons - lonbounds[1] ) )  \n",
    "    # slice the data\n",
    "    \n",
    "    new_lat = dataset.variables['lat'][latli:latui]\n",
    "    new_lon = dataset.variables['lon'][lonli:lonui]\n",
    "    \n",
    "    if return_lat_lon:\n",
    "        return dataset.variables[var][ latli:latui , lonli:lonui ], new_lat, new_lon\n",
    "    else:\n",
    "        return dataset.variables[var][ latli:latui , lonli:lonui ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_netCDFfile(filen, dim_source = None, lat = None, lon = None, data = None, name = 'name', format = 'f4', units = '', long_name = 'Value'):\n",
    "    # if the file exists, delete it\n",
    "    if os.path.exists(filen):\n",
    "        os.remove(filen)\n",
    "\n",
    "    # create the netcdf file\n",
    "    dataset = nc.Dataset(filen, 'w', format='NETCDF4')\n",
    "    # create the dimensions\n",
    "    if dim_source is None:\n",
    "        dataset.createDimension('lat', len(lat))\n",
    "        dataset.createDimension('lon', len(lon))\n",
    "        \n",
    "        if lat is not None:\n",
    "            latitude = dataset.createVariable('lat', 'f4', ('lat',))\n",
    "            # add the attributes\n",
    "            latitude.units = 'degrees_north'\n",
    "            latitude.long_name = 'latitude'\n",
    "            # add the data to the variables\n",
    "            latitude[:] = lat\n",
    "            \n",
    "        \n",
    "        if lon is not None:\n",
    "            longitude = dataset.createVariable('lon', 'f4', ('lon',))\n",
    "            longitude.units = 'degrees_east'\n",
    "            longitude.long_name = 'longitude'\n",
    "            longitude[:] = lon\n",
    "    else:\n",
    "         # copy the dimensions and variables from the source file\n",
    "        for dname, the_dim in dim_source.dimensions.items():\n",
    "            dataset.createDimension(dname, len(the_dim) if not the_dim.isunlimited() else None)\n",
    "        for v_name, varin in dim_source.variables.items():\n",
    "            # only create the variables that are dimensions in the source file\n",
    "            if v_name in dim_source.dimensions:\n",
    "                outVar = dataset.createVariable(v_name, varin.datatype, varin.dimensions)\n",
    "                # Copy variable attributes\n",
    "                outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "                outVar[:] = varin[:]\n",
    "    \n",
    "    \n",
    "    # create a geogreferenced variable\n",
    "    if data is not None:\n",
    "        newVar = dataset.createVariable(name, format, ('lat', 'lon'))\n",
    "        # add the attributes\n",
    "        newVar.units = units\n",
    "        newVar.long_name = long_name\n",
    "        # add the lat and lon variables\n",
    "        \n",
    "        newVar[:] = data\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def add_variable_to_dataset(dataset, variable_data, name = 'value', units = '', fmt = 'f4', dimensions = ('lat', 'lon'), long_name = 'Value'):\n",
    "    \"\"\"\n",
    "    Add a variable to a dataset\n",
    "    \"\"\"\n",
    "    # create a geogreferenced variable\n",
    "    newVar = dataset.createVariable(name, fmt, dimensions)\n",
    "    # add the attributes\n",
    "    newVar.units = units\n",
    "    newVar.long_name = long_name\n",
    "    # add the lat and lon variables\n",
    "    newVar[:] = variable_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new netcdf file\n",
    "# create the netcdf file\n",
    "with nc.Dataset(filenames[0], 'r', format='NETCDF4') as dim_source:\n",
    "    new_netcdf = create_basic_netCDFfile('new_netcdf.nc', dim_source=dim_source)\n",
    "\n",
    "# create the variables\n",
    "vars = {\n",
    "    'mean': (mean, 'Mean Cloud Fraction'), \n",
    "    'median': (median, 'Median Cloud Fraction'),\n",
    "    'quantile_75': (quantile_75, '75th Percentile Cloud Fraction'),\n",
    "    'quantile_25': (quantile_25, '25th Percentile Cloud Fraction')\n",
    "}\n",
    "\n",
    "for var in vars:\n",
    "    add_variable_to_dataset(new_netcdf, vars[var][0], name=var, units='percentage', dimensions=('lat', 'lon'), long_name=vars[var][1])\n",
    "\n",
    "# new_netcdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "median\n",
      "quantile_75\n",
      "quantile_25\n"
     ]
    }
   ],
   "source": [
    "latbounds = [14, 52]\n",
    "lonbounds = [-130, -60]\n",
    "\n",
    "# Slice the data and create variables for each variable in the vars dictionary\n",
    "sliced_data = {}\n",
    "for var in vars:\n",
    "    sliced_data[var] = slice_data(new_netcdf, var, latbounds=latbounds, lonbounds=lonbounds, return_lat_lon=False)\n",
    "\n",
    "_, sliced_lat, sliced_lon = slice_data(new_netcdf, 'mean', latbounds=latbounds, lonbounds=lonbounds, return_lat_lon=True)\n",
    "# Create a new netcdf file and add the sliced variables to it\n",
    "sliced_dataset = create_basic_netCDFfile('new_netcdf2.nc', \n",
    "                                  lat = sliced_lat,\n",
    "                                  lon = sliced_lon,)\n",
    "\n",
    "for var in vars:\n",
    "    print(var)\n",
    "    add_variable_to_dataset(sliced_dataset, sliced_data[var], name=var, units='percentage', fmt='f4', dimensions=('lat', 'lon'))\n",
    "\n",
    "\n",
    "sliced_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr = np.zeros((len(sliced_lat)+1, len(sliced_lon)+1))\n",
    "out_arr[0,0] = 9999\n",
    "out_arr[1:,1:] = sliced_data['mean']\n",
    "out_arr[0,1:] = sliced_lon\n",
    "out_arr[1:,0] = sliced_lat\n",
    "\n",
    "# write out out_arr to a csv file, with trailing zeros trimmed\n",
    "np.savetxt('out.csv', out_arr, delimiter=',', fmt='%0.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
